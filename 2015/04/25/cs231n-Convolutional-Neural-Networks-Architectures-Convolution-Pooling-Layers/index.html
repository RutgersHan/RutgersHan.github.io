
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  
    <title>[cs231n]Convolutional Neural Networks: Architectures, Convolution / Pooling Layers | Be a geek</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="Han Zhang">
    

    
    <meta name="description" content="Lecture NoteReferences:

CNN in Matlab
DeepLearning.net tutorial walks through an implementation of a ConvNet in Theano
cuda-convnet2 by Alex Krizhevsky is a ConvNet implementation that supports multi">
<meta property="og:type" content="article">
<meta property="og:title" content="[cs231n]Convolutional Neural Networks: Architectures, Convolution / Pooling Layers">
<meta property="og:url" content="https://RutgersHan.github.io/2015/04/25/cs231n-Convolutional-Neural-Networks-Architectures-Convolution-Pooling-Layers/index.html">
<meta property="og:site_name" content="Be a geek">
<meta property="og:description" content="Lecture NoteReferences:

CNN in Matlab
DeepLearning.net tutorial walks through an implementation of a ConvNet in Theano
cuda-convnet2 by Alex Krizhevsky is a ConvNet implementation that supports multi">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="[cs231n]Convolutional Neural Networks: Architectures, Convolution / Pooling Layers">
<meta name="twitter:description" content="Lecture NoteReferences:

CNN in Matlab
DeepLearning.net tutorial walks through an implementation of a ConvNet in Theano
cuda-convnet2 by Alex Krizhevsky is a ConvNet implementation that supports multi">

    
    <link rel="alternative" href="/atom.xml" title="Be a geek" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/victor.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/victor.jpg">
    
    <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>

  <body>
    <header>
      <div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.svg" alt="Be a geek" title="Be a geek"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Be a geek">Be a geek</a></h1>
				<h2 class="blog-motto">梦想一定要有的，万一见鬼了呢</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="搜索" />
						<input type="hidden" name="q" value="site:RutgersHan.github.io">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2015/04/25/cs231n-Convolutional-Neural-Networks-Architectures-Convolution-Pooling-Layers/" title="[cs231n]Convolutional Neural Networks: Architectures, Convolution / Pooling Layers" itemprop="url">[cs231n]Convolutional Neural Networks: Architectures, Convolution / Pooling Layers</a>
  </h1>
  <p class="article-author">By
       
		<a href="https://RutgersHan.github.io/about" title="Han Zhang" target="_blank" itemprop="author">Han Zhang</a>
		
  <p class="article-time">
    <time datetime="2015-04-26T02:12:58.000Z" itemprop="datePublished"> 发表于 2015-04-25</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">文章目录</strong>
		
			<ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Convolutional_Layer"><span class="toc-number">1.</span> <span class="toc-text">Convolutional Layer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pooling_Layer"><span class="toc-number">2.</span> <span class="toc-text">Pooling Layer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Normalization_Layer"><span class="toc-number">3.</span> <span class="toc-text">Normalization Layer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Fully-connected_layer"><span class="toc-number">4.</span> <span class="toc-text">Fully-connected layer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ConvNet_Architectures"><span class="toc-number">5.</span> <span class="toc-text">ConvNet Architectures</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Computational_Considerations"><span class="toc-number">6.</span> <span class="toc-text">Computational Considerations</span></a></li></ol>
		
		</div>
		
		<p><a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">Lecture Note</a><br>References:</p>
<ul>
<li><a href="http://www.vlfeat.org/matconvnet/" target="_blank" rel="external">CNN in Matlab</a></li>
<li><a href="http://deeplearning.net/tutorial/lenet.html" target="_blank" rel="external">DeepLearning.net tutorial</a> walks through an implementation of a ConvNet in Theano</li>
<li><a href="https://code.google.com/p/cuda-convnet2/" target="_blank" rel="external">cuda-convnet2</a> by Alex Krizhevsky is a ConvNet implementation that supports multiple GPUs</li>
<li><a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html" target="_blank" rel="external">ConvNetJS CIFAR-10 demo</a> allows you to play with ConvNet architectures and see the results and computations in real time, in the browser.</li>
<li><a href="http://caffe.berkeleyvision.org/" target="_blank" rel="external">Caffe</a>, one of the most popular ConvNet libraries.</li>
<li><a href="https://github.com/nagadomi/kaggle-cifar10-torch7" target="_blank" rel="external">Example Torch 7 ConvNet</a> that achieves 7% error on CIFAR-10 with a single model</li>
<li><a href="https://www.kaggle.com/c/cifar-10/forums/t/10493/train-you-very-own-deep-convolutional-network/56310" target="_blank" rel="external">Ben Graham’s Sparse ConvNet</a> package, which Ben Graham used to great success to achieve less than 4% error on CIFAR-10.</li>
</ul>
<a id="more"></a>
<p>Every word count in this lecture. I should copy the whole lecture here. So whenever you have doubt with CNN. You should go the the original one. Here just writting some points for memorization. </p>
<h3 id="Convolutional_Layer">Convolutional Layer</h3><p><strong>Overview</strong>: The CONV layer’s parameters consist of a set of learnable filters. Every filter is small spatially (along width and height), but extends through the <strong>full depth</strong>(#CNN，每一个小的CNN patch 在长和宽是局部的，但是在depth 一定是整个的input volume的depth！！！) of the input volume.</p>
<p><strong>Use of zero-padding</strong>. In general, setting zero padding to be \(P = (F - 1)/2\) when the stride is \(S = 1\) ensures that the input volume and output volume will have the same size spatially. It is very common to use zero-padding in this way.<br>In numpy the code is like this:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">padded_x = np.pad(x,((<span class="number">0</span>,<span class="number">0</span>),(<span class="number">0</span>,<span class="number">0</span>),(pad,pad),(pad,pad)),<span class="string">'constant'</span>,constant_values = <span class="number">0</span>)</span><br></pre></td></tr></table></figure></p>
<p><strong>Constraints on strides</strong>. Note that the spatial arrangement hyperparameters have mutual constraints. For example, when the input has size \(W = 10\), no zero-padding is used \(P = 0\), and the filter size is \(F = 3\), then it would be impossible to use stride \(S = 2\), since \((W - F + 2P)/S + 1 = (10 - 3 + 0) / 2 + 1 = 4.5\), i.e. not an integer, indicating that the neurons don’t “fit” neatly and symmetrically across the input. Therefore, this setting of the hyperparameters is considered to be invalid, and a ConvNet library would likely throw an exception. </p>
<p><strong>Parameter Sharing</strong>: CNN assumption is that object can occure anywhere is in the image(General image setting). However, when the input  been centered in the image(face). You might expect that different eye-specific or hair-specific features could (and should) be learned in different spatial locations. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a Locally-Connected Layer.</p>
<p><strong>Implementation as Matrix Multiplication, Using(im2col) （OR using FFT for efficiency）</strong>.<br>e.g.<br>The local regions in the input image are stretched out into columns in an operation commonly called <strong>im2col</strong>. For example, if the input is [227x227x3] and it is to be convolved with 11x11x3 filters at stride 4, then we would take [11x11x3] blocks of pixels in the input and stretch each block into a column vector of size 11*11*3 = 363. Iterating this process in the input at stride of 4 gives (227-11)/4+1 = 55 locations along both width and height, leading to an output matrix <code>X_col</code> of <em>im2col</em> of size [363 x 3025], where every column is a stretched out receptive field and there are 55*55 = 3025 of them in total. Note that since the receptive fields overlap, every number in the input volume may be duplicated in multiple distinct columns.</p>
<p><strong>Backpropagation.</strong> The backward pass for a convolution opteration (for both the data and the weights) is also a convolution (but with spatially-flipped filters). </p>
<p><strong>Summary</strong>. To summarize, the Conv Layer:</p>
<ul>
<li>Accepts a volume of size \(W_1 \times H_1 \times D_1\)</li>
<li>Requires four hyperparameters: <ul>
<li>Number of filters \(K\), </li>
<li>their spatial extent \(F\), </li>
<li>the stride \(S\), </li>
<li>the amount of zero padding \(P\).</li>
</ul>
</li>
<li>Produces a volume of size \(W_2 \times H_2 \times D_2\) where:<ul>
<li>\(W_2 = (W_1 - F + 2P)/S + 1\)</li>
<li>\(H_2 = (H_1 - F + 2P)/S + 1\) (i.e. width and height are computed equally by symmetry)</li>
<li>\(D_2 = K\)</li>
</ul>
</li>
<li>With parameter sharing, it introduces \(F \cdot F \cdot D_1\) weights per filter, for a total of \((F \cdot F \cdot D_1) \cdot K\) weights and \(K\) biases.</li>
<li>In the output volume, the \(d\)-th depth slice (of size \(W_2 \times H_2\)) is the result of performing a valid convolution of the \(d\)-th filter over the input volume with a stride of \(S\), and then offset by \(d\)-th bias.</li>
</ul>
<p>A common setting of the hyperparameters is \(F = 3, S = 1, P = 1\). However, there are common conventions and rules of thumb that motivate these hyperparameters. </p>
<h3 id="Pooling_Layer">Pooling Layer</h3><p><strong>Overview</strong>:<br>The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation. The most common form is a pooling layer with filters of size 2x2 applied with a stride of 2 downsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations. Every MAX operation would in this case be taking a max over 4 numbers (little 2x2 region in some depth slice). <strong>The depth dimension remains unchanged</strong></p>
<ul>
<li>Accepts a volume of size \(W_1 \times H_1 \times D_1\)</li>
<li>Requires three hyperparameters: <ul>
<li>their spatial extent \(F\), </li>
<li>the stride \(S\), </li>
</ul>
</li>
<li>Produces a volume of size \(W_2 \times H_2 \times D_2\) where:<ul>
<li>\(W_2 = (W_1 - F)/S + 1\)</li>
<li>\(H_2 = (H_1 - F)/S + 1\)</li>
<li>\(D_2 = D_1\)</li>
</ul>
</li>
<li>Introduces zero parameters since it computes a fixed function of the input</li>
<li>Note that it is not common to use zero-padding for Pooling layers<br>It is worth noting that there are only two commonly seen variations of the max pooling layer found in practice: A pooling layer with \(F = 3, S = 2\) (also called overlapping pooling), and more commonly \(F = 2, S = 2\). Pooling sizes with larger receptive fields are too destructive.</li>
</ul>
<p><strong>Backpropagation</strong>. it is common to keep track of the index of the max activation (sometimes also called <em>the switches</em>) so that gradient routing is efficient during backpropagation.</p>
<h3 id="Normalization_Layer">Normalization Layer</h3><p>Used in Alex Net. Don’t use it now, the contribution is minimal. </p>
<h3 id="Fully-connected_layer">Fully-connected layer</h3><p><strong>Converting FC layers to CONV layers</strong> Use a lot in several papers for localization and segmentation.  Please see the original lecture for details!!!!</p>
<h3 id="ConvNet_Architectures">ConvNet Architectures</h3><p>commonly made up of only three layer types: CONV, POOL (we assume Max pool unless stated otherwise) and FC (short for fully-connected). We will also explicitly write the RELU activation function as a layer, which applies elementwise non-linearity</p>
<p>the most common ConvNet architecture follows the pattern:</p>
<p><code>INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]*M -&gt; [FC -&gt; RELU]*K -&gt; FC</code></p>
<p><strong>Prefer a stack of small filter CONV to one large receptive field CONV layer.</strong><br>Stacking CONV layers with tiny filters as opposed to having one CONV layer with big filters allows us to express more powerful features of the input, and with fewer parameters(see the original lecture for real examples<br>). As a practical disadvantage, we might need more memory to hold all the intermediate CONV layer results if we plan to do backpropagation.</p>
<p><strong>all the CONV layers preserve the spatial size of their input, while the POOL layers alone are in charge of down-sampling the volumes spatially</strong></p>
<p><strong>input layer (that contains the image)</strong>: should be divisible by 2 many times. Common numbers include 32 (e.g. CIFAR-10), 64, 96 (e.g. STL-10), or 224 (e.g. common ImageNet ConvNets), 384, and 512. If not, warp the image to the desired size.</p>
<p><strong>conv layers</strong> s using small filters (e.g. 3x3 or at most 5x5), using a stride of \(S = 1\), and crucially, padding the input volume with zeros in such way that the conv layer does not alter the spatial dimensions of the input. That is, when \(F = 3\), then using \(P = 1\) will retain the original size of the input. When \(F = 5\), \(P = 2\). For a general \(F\), it can be seen that \(P = (F - 1) / 2\) preserves the input size. If you must use bigger filter sizes (such as 7x7 or so), it is only common to see this on the very first conv layer that is looking at the input image.</p>
<p><strong>pool layer</strong>  It is very uncommon to see receptive field sizes for max pooling that are larger than or equal to 3 because the pooling is then too lossy and agressive. This usually leads to worse performance.</p>
<p><strong>Compromising based on memory constraints.</strong> Memory build up very quickly,  one compromise might be to use a first CONV layer with filter sizes of 7x7 and stride of 2 (as seen in a ZF net). As another example, an AlexNet uses filer sizes of 11x11 and stride of 4.</p>
<p><strong>Why use padding?</strong> In addition to the aforementioned benefit of keeping the spatial sizes constant after CONV, doing this actually improves performance. If the CONV layers were to not zero-pad the inputs and only perform valid convolutions, then the size of the volumes would reduce by a small amount after each CONV, and the information at the borders would be “washed away” too quickly.</p>
<p><strong>Why use stride of 1 in CONV?</strong>: stride 1 with \(P = (F - 1) / 2\) preserves the input size. Leave all spatial down-sampling to the POOL layers, with the CONV layers only transforming the input volume depth-wise.</p>
<h3 id="Computational_Considerations">Computational Considerations</h3><p>The largest bottleneck to be aware of when constructing ConvNet architectures is the memory bottleneck. Many modern GPUs have a limit of 3/4/6GB memory, with the best GPUs having about 12GB of memory. There are three major sources of memory to keep track of:</p>
<ul>
<li>From the intermediate volume sizes: These are the raw number of <strong>activations</strong> at every layer of the ConvNet, and also their gradients (of equal size). Usually, most of the activations are on the earlier layers of a ConvNet (i.e. first Conv Layers). These are kept around because they are needed for backpropagation, but a clever implementation that runs a ConvNet only at test time could in principle reduce this by a huge amount, by only storing the current activations at any layer and discarding the previous activations on layers below.</li>
<li>From the parameter sizes: These are the numbers that hold the network <strong>parameters</strong>, their gradients during backpropagation, and commonly also a step cache if the optimization is using momentum, Adagrad, or RMSProp. Therefore, the memory to store the parameter vector alone must usually be multiplied by a factor of at least 3 or so.</li>
<li>Every ConvNet implementation has to maintain <strong>miscellaneous</strong> memory, such as the image data batches, perhaps their augmented versions, etc.</li>
</ul>
<p>Once you have a rough estimate of the total number of values (for activations, gradients, and misc), the number should be converted to size in GB. Take the number of values, multiply by 4 to get the raw number of bytes (since every floating point is 4 bytes, or maybe by 8 for double precision), and then divide by 1024 multiple times to get the amount of memory in KB, MB, and finally GB. If your network doesn’t fit, a common heuristic to “make it fit” is to decrease the batch size, since most of the memory is usually consumed by the activations.</p>
  
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/Open-Course/">Open Course</a>
</div>


</div>



	<div class="article-share" id="share">
	
	  <div data-url="https://RutgersHan.github.io/2015/04/25/cs231n-Convolutional-Neural-Networks-Architectures-Convolution-Pooling-Layers/" data-title="[cs231n]Convolutional Neural Networks: Architectures, Convolution / Pooling Layers | Be a geek" data-tsina="null" class="share clearfix">
	  </div>
	
	</div>


</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2015/04/26/cs231n-Understanding-and-Visualizing-Convolutional-Neural-Networks/" title="[cs231n]Understanding and Visualizing Convolutional Neural Networks">
  <strong>上一篇：</strong><br/>
  <span>
  [cs231n]Understanding and Visualizing Convolutional Neural Networks</span>
</a>
</div>


<div class="next">
<a href="/2015/04/25/cs231n-Putting-it-together-Minimal-Neural-Network-Case-Study/"  title="[cs231n]Putting it together: Minimal Neural Network Case Study">
 <strong>下一篇：</strong><br/> 
 <span>[cs231n]Putting it together: Minimal Neural Network Case Study
</span>
</a>
</div>

</nav>

	
<section id="comments" class="comment">
	<div class="ds-thread" data-thread-key="2015/04/25/cs231n-Convolutional-Neural-Networks-Architectures-Convolution-Pooling-Layers/" data-title="[cs231n]Convolutional Neural Networks: Architectures, Convolution / Pooling Layers" data-url="https://RutgersHan.github.io/2015/04/25/cs231n-Convolutional-Neural-Networks-Architectures-Convolution-Pooling-Layers/"></div>
</section>


<section id="comments" class="comment">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</div>  
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">文章目录</strong>
 
 <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Convolutional_Layer"><span class="toc-number">1.</span> <span class="toc-text">Convolutional Layer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pooling_Layer"><span class="toc-number">2.</span> <span class="toc-text">Pooling Layer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Normalization_Layer"><span class="toc-number">3.</span> <span class="toc-text">Normalization Layer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Fully-connected_layer"><span class="toc-number">4.</span> <span class="toc-text">Fully-connected layer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ConvNet_Architectures"><span class="toc-number">5.</span> <span class="toc-text">ConvNet Architectures</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Computational_Considerations"><span class="toc-number">6.</span> <span class="toc-text">Computational Considerations</span></a></li></ol>
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  
<div class="categorieslist">
	<p class="asidetitle">分类</p>
		<ul>
		
		  
			<li><a href="/categories/Engineering/" title="Engineering">Engineering<sup>5</sup></a></li>
		  
		
		  
			<li><a href="/categories/LeetCode/" title="LeetCode">LeetCode<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/Open-Course/" title="Open Course">Open Course<sup>17</sup></a></li>
		  
		
		  
			<li><a href="/categories/Research/" title="Research">Research<sup>8</sup></a></li>
		  
		
		  
			<li><a href="/categories/计划/" title="计划">计划<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/读书笔记/" title="读书笔记">读书笔记<sup>1</sup></a></li>
		  
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">标签</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/Deep-Learning/" title="Deep Learning">Deep Learning<sup>5</sup></a></li>
			
		
			
				<li><a href="/tags/caffe/" title="caffe">caffe<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/时间管理/" title="时间管理">时间管理<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Notes/" title="Notes">Notes<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Object-Detection/" title="Object Detection">Object Detection<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/deep-learning/" title="deep learning">deep learning<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Image-Caption-Generation/" title="Image Caption Generation">Image Caption Generation<sup>1</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
    </ul>
</div>

  


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS 订阅</a>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello ,I&#39;m Han at Rutgers. <br/>
			Wanna be a geek, let&#39;s learn together</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		
		
		
		
		
		
		
		
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2015 
		
		<a href="https://RutgersHan.github.io/about" target="_blank" title="Han Zhang">Han Zhang</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#nothing"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>



<script type="text/javascript">
  var duoshuoQuery = {short_name:"zhanghan"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
    || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script> 


<script type="text/javascript">

var disqus_shortname = 'zhanghan';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>






<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->





<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="返回顶部"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  <!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>
