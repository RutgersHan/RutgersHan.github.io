
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  
    <title>[CS231n]Assignment #1: Image Classification, kNN, SVM, Softmax | Be a geek</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="Han Zhang">
    

    
    <meta name="description" content="KNN, SVM, Softmax">
<meta property="og:type" content="article">
<meta property="og:title" content="[CS231n]Assignment #1: Image Classification, kNN, SVM, Softmax">
<meta property="og:url" content="https://RutgersHan.github.io/2015/04/19/CS231n-Assignment-1-Image-Classification-kNN-SVM-Softmax/index.html">
<meta property="og:site_name" content="Be a geek">
<meta property="og:description" content="KNN, SVM, Softmax">
<meta property="og:image" content="http://7xikhz.com1.z0.glb.clouddn.com/Screen Shot 2015-04-21 at 1.18.24 PM.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="[CS231n]Assignment #1: Image Classification, kNN, SVM, Softmax">
<meta name="twitter:description" content="KNN, SVM, Softmax">

    
    <link rel="alternative" href="/atom.xml" title="Be a geek" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/victor.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/victor.jpg">
    
    <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>

  <body>
    <header>
      <div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.svg" alt="Be a geek" title="Be a geek"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Be a geek">Be a geek</a></h1>
				<h2 class="blog-motto">梦想一定要有的，万一见鬼了呢</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="搜索" />
						<input type="hidden" name="q" value="site:RutgersHan.github.io">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2015/04/19/CS231n-Assignment-1-Image-Classification-kNN-SVM-Softmax/" title="[CS231n]Assignment #1: Image Classification, kNN, SVM, Softmax" itemprop="url">[CS231n]Assignment #1: Image Classification, kNN, SVM, Softmax</a>
  </h1>
  <p class="article-author">By
       
		<a href="https://RutgersHan.github.io/about" title="Han Zhang" target="_blank" itemprop="author">Han Zhang</a>
		
  <p class="article-time">
    <time datetime="2015-04-19T19:50:34.000Z" itemprop="datePublished"> 发表于 2015-04-19</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">文章目录</strong>
		
			<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#KNN"><span class="toc-number">1.</span> <span class="toc-text">KNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Vectorized_version_to_compute_distance_for_KNN"><span class="toc-number">1.1.</span> <span class="toc-text">Vectorized version to compute distance for KNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cross_Validation"><span class="toc-number">1.2.</span> <span class="toc-text">Cross Validation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Plot_the_result_for_cross_validation"><span class="toc-number">1.3.</span> <span class="toc-text">Plot the result for cross validation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVM"><span class="toc-number">2.</span> <span class="toc-text">SVM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Vectorized_version_for_multiclass_SVM_to_compute_loss_and_gradient"><span class="toc-number">2.1.</span> <span class="toc-text">Vectorized version for multiclass SVM to compute loss and gradient</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#How_to_get_training_batch"><span class="toc-number">2.2.</span> <span class="toc-text">How to get training batch</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#How_to_tune_Hyperparameter"><span class="toc-number">2.3.</span> <span class="toc-text">How to tune Hyperparameter</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Results_of_SVM"><span class="toc-number">2.4.</span> <span class="toc-text">Results of SVM</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SoftMAX"><span class="toc-number">3.</span> <span class="toc-text">SoftMAX</span></a></li></ol>
		
		</div>
		
		<p>KNN, SVM, Softmax<br><a id="more"></a></p>
<h2 id="KNN">KNN</h2><h3 id="Vectorized_version_to_compute_distance_for_KNN">Vectorized version to compute distance for KNN</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_distances_no_loops</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    Compute the distance between each test point in X and each training point</span><br><span class="line">    in self.X_train using no explicit loops.</span><br><span class="line"></span><br><span class="line">    Input / Output: Same as compute_distances_two_loops</span><br><span class="line">    """</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">    dists = np.zeros((num_test, num_train)) </span><br><span class="line">    <span class="comment">#########################################################################</span></span><br><span class="line">    <span class="comment"># TODO:                                                                 #</span></span><br><span class="line">    <span class="comment"># Compute the l2 distance between all test points and all training      #</span></span><br><span class="line">    <span class="comment"># points without using any explicit loops, and store the result in      #</span></span><br><span class="line">    <span class="comment"># dists.                                                                #</span></span><br><span class="line">    <span class="comment"># HINT: Try to formulate the l2 distance using matrix multiplication    #</span></span><br><span class="line">    <span class="comment">#       and two broadcast sums.                                         #</span></span><br><span class="line">    <span class="comment">#########################################################################</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">    cross_product = X.dot(self.X_train.T);</span><br><span class="line">    test_squared = np.sum(X**<span class="number">2</span>, axis=<span class="number">1</span>)</span><br><span class="line">    train_squared = np.sum(self.X_train**<span class="number">2</span>, axis=<span class="number">1</span>)</span><br><span class="line">    temp = -<span class="number">2</span> * cross_product + train_squared.reshape((<span class="number">1</span>, num_train))</span><br><span class="line">    temp = temp + test_squared.reshape((num_test,<span class="number">1</span>));</span><br><span class="line">    dists = np.sqrt(temp)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#########################################################################</span></span><br><span class="line">    <span class="comment">#                         END OF YOUR CODE                              #</span></span><br><span class="line">    <span class="comment">#########################################################################</span></span><br><span class="line">    <span class="keyword">return</span> dists</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let's compare how fast the implementations are</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_function</span><span class="params">(f, *args)</span>:</span></span><br><span class="line">  <span class="string">"""</span><br><span class="line">  Call a function f with args and return the time (in seconds) that it took to execute.</span><br><span class="line">  """</span></span><br><span class="line">  <span class="keyword">import</span> time</span><br><span class="line">  tic = time.time()</span><br><span class="line">  f(*args)</span><br><span class="line">  toc = time.time()</span><br><span class="line">  <span class="keyword">return</span> toc - tic</span><br><span class="line"></span><br><span class="line">two_loop_time = time_function(classifier.compute_distances_two_loops, X_test)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'Two loop version took %f seconds'</span> % two_loop_time</span><br><span class="line"></span><br><span class="line">one_loop_time = time_function(classifier.compute_distances_one_loop, X_test)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'One loop version took %f seconds'</span> % one_loop_time</span><br><span class="line"></span><br><span class="line">no_loop_time = time_function(classifier.compute_distances_no_loops, X_test)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'No loop version took %f seconds'</span> % no_loop_time</span><br><span class="line"></span><br><span class="line"><span class="comment"># you should see significantly faster performance with the fully vectorized implementation</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>Two loop version took 70.987422 seconds<br>One loop version took 50.101490 seconds<br>No loop version took 0.824264 seconds</p>
</blockquote>
<h3 id="Cross_Validation">Cross Validation</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">num_folds = <span class="number">5</span></span><br><span class="line">k_choices = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">50</span>, <span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">X_train_folds = []</span><br><span class="line">y_train_folds = []</span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># TODO:                                                                        #</span></span><br><span class="line"><span class="comment"># Split up the training data into folds. After splitting, X_train_folds and    #</span></span><br><span class="line"><span class="comment"># y_train_folds should each be lists of length num_folds, where                #</span></span><br><span class="line"><span class="comment"># y_train_folds[i] is the label vector for the points in X_train_folds[i].     #</span></span><br><span class="line"><span class="comment"># Hint: Look up the numpy array_split function.                                #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="keyword">pass</span></span><br><span class="line">nums_train = X_train.shape[<span class="number">0</span>]</span><br><span class="line">orders = np.random.permutation(nums_train)</span><br><span class="line">X_train_folds = np.array_split(X_train[orders],num_folds,axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">y_train_orders  = y_train[orders];</span><br><span class="line">y_train_orders = y_train_orders.reshape((nums_train,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">y_train_folds = np.array_split(y_train_orders,num_folds,axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#                                 END OF YOUR CODE                             #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A dictionary holding the accuracies for different values of k that we find</span></span><br><span class="line"><span class="comment"># when running cross-validation. After running cross-validation,</span></span><br><span class="line"><span class="comment"># k_to_accuracies[k] should be a list of length num_folds giving the different</span></span><br><span class="line"><span class="comment"># accuracy values that we found when using that value of k.</span></span><br><span class="line">k_to_accuracies = &#123;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># TODO:                                                                        #</span></span><br><span class="line"><span class="comment"># Perform k-fold cross validation to find the best value of k. For each        #</span></span><br><span class="line"><span class="comment"># possible value of k, run the k-nearest-neighbor algorithm num_folds times,   #</span></span><br><span class="line"><span class="comment"># where in each case you use all but one of the folds as training data and the #</span></span><br><span class="line"><span class="comment"># last fold as a validation set. Store the accuracies for all fold and all     #</span></span><br><span class="line"><span class="comment"># values of k in the k_to_accuracies dictionary.                               #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_folds):</span><br><span class="line">    t_indexs = range(num_folds);</span><br><span class="line">    t_indexs.remove(i)</span><br><span class="line">    c_X_train = np.vstack([X_train_folds[j] <span class="keyword">for</span> j <span class="keyword">in</span> t_indexs])</span><br><span class="line">    c_y_train = np.vstack([y_train_folds[j] <span class="keyword">for</span> j <span class="keyword">in</span> t_indexs])</span><br><span class="line">    </span><br><span class="line">    c_X_test = X_train_folds[i]</span><br><span class="line">    c_y_test = y_train_folds[i]</span><br><span class="line">    num_test = c_y_test.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</span><br><span class="line">        <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> k_to_accuracies:</span><br><span class="line">            k_to_accuracies[k] = []</span><br><span class="line">        classifier = KNearestNeighbor()</span><br><span class="line">        classifier.train(c_X_train, c_y_train)</span><br><span class="line">        dists = classifier.compute_distances_no_loops(c_X_test)</span><br><span class="line">        y_test_pred = classifier.predict_labels(dists, k)</span><br><span class="line">        y_test_pred = y_test_pred.reshape((num_test,<span class="number">1</span>))</span><br><span class="line">        num_correct = np.sum(y_test_pred == c_y_test)</span><br><span class="line">        accuracy = float(num_correct) / num_test</span><br><span class="line">        k_to_accuracies[k].append(accuracy)</span><br><span class="line">        </span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#                                 END OF YOUR CODE                             #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the computed accuracies</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> sorted(k_to_accuracies):</span><br><span class="line">    <span class="keyword">for</span> accuracy <span class="keyword">in</span> k_to_accuracies[k]:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'k = %d, accuracy = %f'</span> % (k, accuracy)</span><br></pre></td></tr></table></figure>
<h3 id="Plot_the_result_for_cross_validation">Plot the result for cross validation</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot the raw observations</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</span><br><span class="line">  accuracies = k_to_accuracies[k]</span><br><span class="line">  plt.scatter([k] * len(accuracies), accuracies)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the trend line with error bars that correspond to standard deviation</span></span><br><span class="line">accuracies_mean = np.array([np.mean(v) <span class="keyword">for</span> k,v <span class="keyword">in</span> sorted(k_to_accuracies.items())])</span><br><span class="line">accuracies_std = np.array([np.std(v) <span class="keyword">for</span> k,v <span class="keyword">in</span> sorted(k_to_accuracies.items())])</span><br><span class="line">plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std)</span><br><span class="line">plt.title(<span class="string">'Cross-validation on k'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'k'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Cross-validation accuracy'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="SVM">SVM</h2><h3 id="Vectorized_version_for_multiclass_SVM_to_compute_loss_and_gradient"><strong>Vectorized version for multiclass SVM to compute loss and gradient</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">  <span class="string">"""</span><br><span class="line">  Structured SVM loss function, vectorized implementation.</span><br><span class="line"></span><br><span class="line">  Inputs and outputs are the same as svm_loss_naive.</span><br><span class="line">  """</span></span><br><span class="line">  loss = <span class="number">0.0</span></span><br><span class="line">  dW = np.zeros(W.shape) <span class="comment"># initialize the gradient as zero</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment"># TODO:                                                                     #</span></span><br><span class="line">  <span class="comment"># Implement a vectorized version of the structured SVM loss, storing the    #</span></span><br><span class="line">  <span class="comment"># result in loss.                                                           #</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="keyword">pass</span></span><br><span class="line">  scores = W.dot(X)</span><br><span class="line">  num_classes = W.shape[<span class="number">0</span>]</span><br><span class="line">  num_train = X.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">  correct_class_score = scores[y, np.arange(num_train)]</span><br><span class="line">  margin = scores - correct_class_score + <span class="number">1</span></span><br><span class="line">  margin[margin &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">  margin[y, np.arange(num_train)] = <span class="number">0</span></span><br><span class="line">  loss = np.sum(margin) / num_train</span><br><span class="line">  loss += <span class="number">0.5</span> * reg * np.sum(W * W)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment">#                             END OF YOUR CODE                              #</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment"># TODO:                                                                     #</span></span><br><span class="line">  <span class="comment"># Implement a vectorized version of the gradient for the structured SVM     #</span></span><br><span class="line">  <span class="comment"># loss, storing the result in dW.                                           #</span></span><br><span class="line">  <span class="comment">#                                                                           #</span></span><br><span class="line">  <span class="comment"># Hint: Instead of computing the gradient from scratch, it may be easier    #</span></span><br><span class="line">  <span class="comment"># to reuse some of the intermediate values that you used to compute the     #</span></span><br><span class="line">  <span class="comment"># loss.                                                                     #</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="keyword">pass</span></span><br><span class="line">  margin_flag = margin&gt;<span class="number">0</span>;</span><br><span class="line">  term2 = margin_flag.dot(X.T)</span><br><span class="line"></span><br><span class="line">  margin_flag_sum = np.sum(margin_flag,axis=<span class="number">0</span>)</span><br><span class="line">  coeff_matrix = np.zeros((num_classes,num_train))</span><br><span class="line">  coeff_matrix[y, np.arange(num_train)] = margin_flag_sum</span><br><span class="line">  term1 = -coeff_matrix.dot(X.T)</span><br><span class="line">  dW = term1 + term2</span><br><span class="line">  dW = dW / num_train</span><br><span class="line">  dW += reg * W</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment">#                             END OF YOUR CODE                              #</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure>
<h3 id="How_to_get_training_batch">How to get training batch</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> xrange(num_iters):</span><br><span class="line">   X_batch = <span class="keyword">None</span></span><br><span class="line">   y_batch = <span class="keyword">None</span></span><br><span class="line">   indexs = np.random.choice(num_train,batch_size,<span class="keyword">False</span>)</span><br><span class="line">   y_batch = y[indexs]</span><br><span class="line">   X_batch = X[:,indexs]</span><br><span class="line">   <span class="comment"># evaluate loss and gradient</span></span><br><span class="line">   loss, grad = self.loss(X_batch, y_batch, reg)</span><br><span class="line">   loss_history.append(loss)</span><br></pre></td></tr></table></figure>
<h3 id="How_to_tune_Hyperparameter">How to tune Hyperparameter</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">learning_rates = [<span class="number">1e-7</span>, <span class="number">1e-6</span>,<span class="number">1e-5</span>]</span><br><span class="line">regularization_strengths = [<span class="number">1e3</span>, <span class="number">5e4</span>, <span class="number">1e5</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># results is dictionary mapping tuples of the form</span></span><br><span class="line"><span class="comment"># (learning_rate, regularization_strength) to tuples of the form</span></span><br><span class="line"><span class="comment"># (training_accuracy, validation_accuracy). The accuracy is simply the fraction</span></span><br><span class="line"><span class="comment"># of data points that are correctly classified.</span></span><br><span class="line">results = &#123;&#125;</span><br><span class="line">best_val = -<span class="number">1</span>   <span class="comment"># The highest validation accuracy that we have seen so far.</span></span><br><span class="line">best_svm = <span class="keyword">None</span> <span class="comment"># The LinearSVM object that achieved the highest validation rate.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> learning_rate <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">for</span> reg <span class="keyword">in</span> regularization_strengths:</span><br><span class="line">        v_svm = LinearSVM()</span><br><span class="line">        loss_hist = v_svm.train(X_train, y_train, learning_rate, reg,</span><br><span class="line">                      num_iters=<span class="number">1500</span>, verbose=<span class="keyword">True</span>)</span><br><span class="line">        y_train_pred = v_svm.predict(X_train)</span><br><span class="line">        y_val_pred = v_svm.predict(X_val)</span><br><span class="line">        y_train_accuracy = np.mean(y_train == y_train_pred)</span><br><span class="line">        y_val_accuracy = np.mean(y_val == y_val_pred)</span><br><span class="line">        results[(learning_rate,reg)] = (y_train_accuracy, y_val_accuracy)</span><br><span class="line">        <span class="keyword">if</span> y_val_accuracy&gt;best_val:</span><br><span class="line">            best_val = y_val_accuracy</span><br><span class="line">            best_svm = v_svm</span><br></pre></td></tr></table></figure>
<h3 id="Results_of_SVM">Results of SVM</h3><p>linear SVM on raw pixels final test set accuracy: 0.375000<br><img src="http://7xikhz.com1.z0.glb.clouddn.com/Screen Shot 2015-04-21 at 1.18.24 PM.png" alt=""></p>
<h2 id="SoftMAX">SoftMAX</h2><p>For the loss of SoftMax  it is easy. Just add up the loss of all samples.<br>To compute the gradient of W in Softmax, we need to do some math. Similiar as SVM, for dW it contains two parts</p>
<p>part1:<br>$$<br>\nabla_{w_{y_i}} L_i = (\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} } - 1) \times x_i<br>$$<br>part2: </p>
<p>$$<br>\nabla_{w_{h \neq y_i}} L_i = (\frac{e^{f_{h}}}{ \sum_j e^{f_j} }) \times x_i<br>$$</p>
<p>where $f_h = w_h x_i$</p>
<p>The vectorized verion to compute the loss and gradient.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">  <span class="string">"""</span><br><span class="line">  Softmax loss function, vectorized version.</span><br><span class="line"></span><br><span class="line">  Inputs and outputs are the same as softmax_loss_naive.</span><br><span class="line">  """</span></span><br><span class="line">  <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">  loss = <span class="number">0.0</span></span><br><span class="line">  dW = np.zeros_like(W)</span><br><span class="line">  num_classes = W.shape[<span class="number">0</span>]</span><br><span class="line">  num_train = X.shape[<span class="number">1</span>]</span><br><span class="line">  coeff1 = np.zeros((num_classes,num_train))</span><br><span class="line"></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment"># TODO: Compute the softmax loss and its gradient using no explicit loops.  #</span></span><br><span class="line">  <span class="comment"># Store the loss in loss and the gradient in dW. If you are not careful     #</span></span><br><span class="line">  <span class="comment"># here, it is easy to run into numeric instability. Don't forget the        #</span></span><br><span class="line">  <span class="comment"># regularization!                                                           #</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="keyword">pass</span></span><br><span class="line">  scores = W.dot(X)</span><br><span class="line">  <span class="comment">#numeric stability</span></span><br><span class="line">  scores = scores - scores.max(<span class="number">0</span>)</span><br><span class="line">  exp_scores = np.exp(scores)</span><br><span class="line">  sum_exp_scores = exp_scores.sum(<span class="number">0</span>)</span><br><span class="line">  normalized_scores = exp_scores / (sum_exp_scores)</span><br><span class="line">  normalized_correct_scores = normalized_scores[y,np.arange(num_train)]</span><br><span class="line">  log_cost = -np.log(normalized_correct_scores + <span class="number">1e-5</span>)</span><br><span class="line">  loss =  np.sum(log_cost) / num_train + <span class="number">0.5</span> * reg * np.sum(W * W) </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">  coeff2 = normalized_scores.copy()</span><br><span class="line">  coeff2[y,np.arange(num_train)] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  dW_part2 = coeff2.dot(X.T)</span><br><span class="line">  coeff1[y,np.arange(num_train)] = normalized_correct_scores - <span class="number">1</span></span><br><span class="line">  dW_part1 = coeff1.dot(X.T)</span><br><span class="line">  dW =  (dW_part1 + dW_part2) / num_train + reg * W</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment">#                          END OF YOUR CODE                                 #</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure></p>
  
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/Open-Course/">Open Course</a>
</div>


</div>



	<div class="article-share" id="share">
	
	  <div data-url="https://RutgersHan.github.io/2015/04/19/CS231n-Assignment-1-Image-Classification-kNN-SVM-Softmax/" data-title="[CS231n]Assignment #1: Image Classification, kNN, SVM, Softmax | Be a geek" data-tsina="null" class="share clearfix">
	  </div>
	
	</div>


</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2015/04/21/cs231n-Neural-Networks-Part-1-Setting-up-the-Architecture/" title="[cs231n]Neural Networks Part 1: Setting up the Architecture ">
  <strong>上一篇：</strong><br/>
  <span>
  [cs231n]Neural Networks Part 1: Setting up the Architecture </span>
</a>
</div>


<div class="next">
<a href="/2015/04/19/Some-common-functions-in-Numpy-and-Scipy/"  title="Some common functions in Numpy and Scipy ">
 <strong>下一篇：</strong><br/> 
 <span>Some common functions in Numpy and Scipy 
</span>
</a>
</div>

</nav>

	
<section id="comments" class="comment">
	<div class="ds-thread" data-thread-key="2015/04/19/CS231n-Assignment-1-Image-Classification-kNN-SVM-Softmax/" data-title="[CS231n]Assignment #1: Image Classification, kNN, SVM, Softmax" data-url="https://RutgersHan.github.io/2015/04/19/CS231n-Assignment-1-Image-Classification-kNN-SVM-Softmax/"></div>
</section>


<section id="comments" class="comment">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

</div>  
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">文章目录</strong>
 
 <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#KNN"><span class="toc-number">1.</span> <span class="toc-text">KNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Vectorized_version_to_compute_distance_for_KNN"><span class="toc-number">1.1.</span> <span class="toc-text">Vectorized version to compute distance for KNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cross_Validation"><span class="toc-number">1.2.</span> <span class="toc-text">Cross Validation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Plot_the_result_for_cross_validation"><span class="toc-number">1.3.</span> <span class="toc-text">Plot the result for cross validation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVM"><span class="toc-number">2.</span> <span class="toc-text">SVM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Vectorized_version_for_multiclass_SVM_to_compute_loss_and_gradient"><span class="toc-number">2.1.</span> <span class="toc-text">Vectorized version for multiclass SVM to compute loss and gradient</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#How_to_get_training_batch"><span class="toc-number">2.2.</span> <span class="toc-text">How to get training batch</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#How_to_tune_Hyperparameter"><span class="toc-number">2.3.</span> <span class="toc-text">How to tune Hyperparameter</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Results_of_SVM"><span class="toc-number">2.4.</span> <span class="toc-text">Results of SVM</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SoftMAX"><span class="toc-number">3.</span> <span class="toc-text">SoftMAX</span></a></li></ol>
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  
<div class="categorieslist">
	<p class="asidetitle">分类</p>
		<ul>
		
		  
			<li><a href="/categories/Engineering/" title="Engineering">Engineering<sup>5</sup></a></li>
		  
		
		  
			<li><a href="/categories/LeetCode/" title="LeetCode">LeetCode<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/Open-Course/" title="Open Course">Open Course<sup>17</sup></a></li>
		  
		
		  
			<li><a href="/categories/Research/" title="Research">Research<sup>8</sup></a></li>
		  
		
		  
			<li><a href="/categories/计划/" title="计划">计划<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/读书笔记/" title="读书笔记">读书笔记<sup>1</sup></a></li>
		  
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">标签</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/Deep-Learning/" title="Deep Learning">Deep Learning<sup>5</sup></a></li>
			
		
			
				<li><a href="/tags/caffe/" title="caffe">caffe<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/时间管理/" title="时间管理">时间管理<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Notes/" title="Notes">Notes<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Object-Detection/" title="Object Detection">Object Detection<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/deep-learning/" title="deep learning">deep learning<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Image-Caption-Generation/" title="Image Caption Generation">Image Caption Generation<sup>1</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
    </ul>
</div>

  


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS 订阅</a>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello ,I&#39;m Han at Rutgers. <br/>
			Wanna be a geek, let&#39;s learn together</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		
		
		
		
		
		
		
		
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2015 
		
		<a href="https://RutgersHan.github.io/about" target="_blank" title="Han Zhang">Han Zhang</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#nothing"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>



<script type="text/javascript">
  var duoshuoQuery = {short_name:"zhanghan"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
    || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script> 


<script type="text/javascript">

var disqus_shortname = 'zhanghan';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>






<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->





<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="返回顶部"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  <!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>
